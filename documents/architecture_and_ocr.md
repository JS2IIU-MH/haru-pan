# アーキテクチャとOCR方針（高レベル設計）

## 目的

- `harupan` の実装方針を整理し、画像前処理・シール検出・OCRの技術選定と実装手順を明確にする。

## システム構成（高レベル）

- Flutter UI 層（`camera` プレビュー、結果表示、編集UI）
- 画像処理層（赤色領域検出、形状フィルタ、領域切り出し）
- OCR 層（領域ごとの文字認識、複数桁対応）
- 集計・表示層（認識値の合計、編集・再読み取り）

処理は端末内で完結（オフライン）する。

## 検出＋認識のパイプライン

1. 入力
   - ライブカメラ／撮影画像／ギャラリー画像
2. 前処理
   - リサイズ（性能と精度のバランス）
   - 色空間変換（RGB → HSV）
   - 赤色マスク作成（閾値で二値化）
   - ノイズ除去（モルフォロジー開閉処理）
3. 領域抽出
   - 輪郭検出／連結成分解析で候補領域を取得
   - 面積、縦横比、円形度でフィルタリング（赤い丸を想定）
4. 領域補正と切り出し
   - 最小外接円や最小外接矩形で切り出し
   - 傾き補正・パディング
5. OCR 実行
   - 領域ごとにテキスト抽出（複数桁対応）
6. 集計と表示
   - 抽出した数値をパースし合計を算出
   - ユーザーに一覧表示。手動修正可能にする（拡張）

## OCR／検出の技術選定候補（比較）

- 方式A（推奨、初期MVP）: 色ベース検出 + Google ML Kit（オンデバイス）
  - 長所: 精度が比較的高く、セットアップが容易。ML Kit のテキスト認識は日本語・数字対応が良好でオンデバイスで動く。
  - 短所: 色依存のため極端な照明や色変化に弱い。

- 方式B: OpenCV（ネイティブ）で高度な前処理 + ML Kit/Tesseract
  - 長所: 色や形の検出を細かく制御できる。難しいケースの耐性が高い。
  - 短所: ネイティブ依存（Android NDKやプラグインのビルド）で実装コストが上がる。

- 方式C: TFLite で専用の物体検出モデル（ステッカー検出） + OCR
  - 長所: 学習済みDetectorで堅牢に検出可能（色や形が多様な場合に有利）。
  - 短所: データ収集と学習のコスト、モデルの最適化が必要。

## 推奨アプローチ（MVP）

- まずは方式A：
  - フロー: Flutter 側でカメラ画像を取得 → 軽量な色ベース前処理（Dartの`image`ライブラリやプラットフォームチャネル）で赤領域を抽出 → 領域を切り出して `google_mlkit_text_recognition`（オンデバイス）でOCR。
  - 理由: 実装コストとオンデバイス要件のバランスが良く、短期間で実装可能。

## 必要ライブラリ候補（Flutter）

- カメラ: `camera` または `camera_plus`
- 画像操作: `image`（Dart純正）またはネイティブで OpenCV を組み込む場合はプラグイン実装
- OCR: `google_mlkit_text_recognition`（オンデバイス）または `tesseract_ocr`（精度調整が必要）

## 性能・精度上の注意点

- 前処理で解像度を下げすぎるとOCR精度が落ちる。切り出し領域はOCRに十分なピクセル数を確保する。
- 照明／反射への耐性を上げるには、色マスクの閾値を動的適応させる（ヒストグラムやホワイトバランス推定）
- リアルタイムモードでは、毎フレームOCRを走らせず、検出のみ軽量に行って撮影時にOCRを実行する。

## テスト方針（評価指標）

- 検出率（赤丸が検出領域として抽出される割合）
- 認識率（正しく数値を読み取れる割合）
- レイテンシ（撮影から合計表示までの時間）

## 次工程（推奨作業）

1. MVP 実装方針で必要な Flutter パッケージを確定し `pubspec.yaml` に追加
2. カメラプレビュー画面と静止画取得の実装
3. 単一画像での前処理→検出→OCR のパイプライン実装とユニットテスト
4. ライブモードの最適化（検出軽量化、バッファ処理）
5. UI（結果一覧、手動修正、再解析）実装
6. テストデータを使った精度評価と閾値調整

---
この設計で進めて実装計画を作成します。次は「最小実装（Android向けFlutter）の開発計画書」を作成します。